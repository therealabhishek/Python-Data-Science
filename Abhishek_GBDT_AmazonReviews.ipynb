{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFYING AMAZON FOOD REVIEWS USING GBDT Classifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "#=============  OBJECTIVE  ============#\n",
    "\n",
    "Classifying Amazon Food Reviews as positive or negative using the GBDT classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries required for our task\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required sqlite database which contains the reviews\n",
    "\n",
    "con = sqlite3.connect('database.sqlite')\n",
    "\n",
    "'''\n",
    "Our objective is to check whether the review is positive or negative.\n",
    "\n",
    "The dataset originally consists of reviews from 1 to 5. We will consider reviews which are rated as 4 and 5 to be positive, reviews \n",
    "which are rated 1 and 2 to be negative. As, we cannot draw any conclusions from review which is rated 3 star we will eliminate\n",
    "all the reviews rated 3 star.\n",
    "\n",
    "'''\n",
    "\n",
    "# Filtering out the data w/o the 3 star reviews\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score != 3\"\"\", con) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 2], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking whether the data has been filtered properly\n",
    "\n",
    "filtered_data['Score'].unique()\n",
    "\n",
    "# We can see that there is no 3 star review in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have eliminated the data with 3 start reviews, we will label the remaining data(4 and 5 scores) as positive and negative(1 and 2 scores).\n",
    "\n",
    "# Creating a function to label the data\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return '0'\n",
    "    return '1'\n",
    "\n",
    "\n",
    "# Applying the labels to the data\n",
    "\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "filtered_data['Score'] = positiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>ADT0SRK1MGOEU</td>\n",
       "      <td>Twoapennything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1342051200</td>\n",
       "      <td>Nice Taffy</td>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1SP2KVKFXXRU1</td>\n",
       "      <td>David C. Sullivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A3JRGQVEQN31IQ</td>\n",
       "      <td>Pamela G. Williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1336003200</td>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>B000E7L2R4</td>\n",
       "      <td>A1MZYO9TZK0BBI</td>\n",
       "      <td>R. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1322006400</td>\n",
       "      <td>Yay Barley</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>B00171APVA</td>\n",
       "      <td>A21BT40VZCCYT4</td>\n",
       "      <td>Carol A. Reed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Healthy Dog Food</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "5   6  B006K2ZZ7K   ADT0SRK1MGOEU                   Twoapennything   \n",
       "6   7  B006K2ZZ7K  A1SP2KVKFXXRU1                David C. Sullivan   \n",
       "7   8  B006K2ZZ7K  A3JRGQVEQN31IQ               Pamela G. Williams   \n",
       "8   9  B000E7L2R4  A1MZYO9TZK0BBI                         R. James   \n",
       "9  10  B00171APVA  A21BT40VZCCYT4                    Carol A. Reed   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator Score        Time  \\\n",
       "0                     1                       1     1  1303862400   \n",
       "1                     0                       0     0  1346976000   \n",
       "2                     1                       1     1  1219017600   \n",
       "3                     3                       3     0  1307923200   \n",
       "4                     0                       0     1  1350777600   \n",
       "5                     0                       0     1  1342051200   \n",
       "6                     0                       0     1  1340150400   \n",
       "7                     0                       0     1  1336003200   \n",
       "8                     1                       1     1  1322006400   \n",
       "9                     0                       0     1  1351209600   \n",
       "\n",
       "                                         Summary  \\\n",
       "0                          Good Quality Dog Food   \n",
       "1                              Not as Advertised   \n",
       "2                          \"Delight\" says it all   \n",
       "3                                 Cough Medicine   \n",
       "4                                    Great taffy   \n",
       "5                                     Nice Taffy   \n",
       "6  Great!  Just as good as the expensive brands!   \n",
       "7                         Wonderful, tasty taffy   \n",
       "8                                     Yay Barley   \n",
       "9                               Healthy Dog Food   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  This is a confection that has been around a fe...  \n",
       "3  If you are looking for the secret ingredient i...  \n",
       "4  Great taffy at a great price.  There was a wid...  \n",
       "5  I got a wild hair for taffy and ordered this f...  \n",
       "6  This saltwater taffy had great flavors and was...  \n",
       "7  This taffy is so good.  It is very soft and ch...  \n",
       "8  Right now I'm mostly just sprouting this so my...  \n",
       "9  This is a very healthy dog food. Good for thei...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first few rows of the data\n",
    "\n",
    "filtered_data.head(10)\n",
    "\n",
    "# We can see that the score has been changed to positive and negative instead of 5,4,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us look at the shape of the data\n",
    "\n",
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping any duplicates if they are present in the data\n",
    "\n",
    "duplicates_dropped=filtered_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "duplicates_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating the rows where helpfulness numerator is greator than the helfulness denominator\n",
    "\n",
    "final=duplicates_dropped[duplicates_dropped.HelpfulnessNumerator<=duplicates_dropped.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at the shape of the data\n",
    "\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will select only the required columns of which we will filter some reviews and assign them their labels and sort them wrt time\n",
    "\n",
    "final_data = final[['ProductId','Time','Text','Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the dimension of the data\n",
    "final_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selecting some reviews from the 'final_data' data\n",
    "\n",
    "# First we will extract the values from the given dataframe\n",
    "\n",
    "X=final_data.iloc[:,:].values\n",
    "\n",
    "# randomly extracting 15000 reviews from the dataset\n",
    "\n",
    "import random\n",
    "\n",
    "n = 364171\n",
    "m = 15000\n",
    "p = m/n\n",
    "\n",
    "sampled_data = [];\n",
    "\n",
    "for i in range(0,n):\n",
    "    if random.random() <= p:\n",
    "        sampled_data.append(X[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the extracted data to a dataframe\n",
    "\n",
    "names = ['ProductId','Time','Text','Score']\n",
    "\n",
    "sample = pd.DataFrame(sampled_data,columns= names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15109, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the dimensions of the sampled data\n",
    "\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B001EO5QW8</td>\n",
       "      <td>1166313600</td>\n",
       "      <td>This is a good instant oatmeal from the best o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B001EO5QW8</td>\n",
       "      <td>1191715200</td>\n",
       "      <td>I really like the Maple and Brown Sugar flavor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0019CW0HE</td>\n",
       "      <td>1333670400</td>\n",
       "      <td>My dog has a ton of allergies both environment...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0019CW0HE</td>\n",
       "      <td>1330041600</td>\n",
       "      <td>My golden retriever is one of the most picky d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId        Time                                               Text  \\\n",
       "0  B006K2ZZ7K  1340150400  This saltwater taffy had great flavors and was...   \n",
       "1  B001EO5QW8  1166313600  This is a good instant oatmeal from the best o...   \n",
       "2  B001EO5QW8  1191715200  I really like the Maple and Brown Sugar flavor...   \n",
       "3  B0019CW0HE  1333670400  My dog has a ton of allergies both environment...   \n",
       "4  B0019CW0HE  1330041600  My golden retriever is one of the most picky d...   \n",
       "\n",
       "  Score  \n",
       "0     1  \n",
       "1     1  \n",
       "2     1  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first few rows of the data\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will sort the data according to timestamp\n",
    "\n",
    "sorted_data=sample.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6986</th>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>1052265600</td>\n",
       "      <td>Embarrassing comedy that comes over like a chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10971</th>\n",
       "      <td>B0000TL5HI</td>\n",
       "      <td>1073606400</td>\n",
       "      <td>I purchased the organic fruit basket as a gift...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7040</th>\n",
       "      <td>B0000DG58Q</td>\n",
       "      <td>1074470400</td>\n",
       "      <td>For many years I tried various meat sauces: Am...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>B0001AGLV6</td>\n",
       "      <td>1074643200</td>\n",
       "      <td>I got some of this blend thinking that it woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6490</th>\n",
       "      <td>B0000DGFAC</td>\n",
       "      <td>1075420800</td>\n",
       "      <td>The plant is very healthy. It arrived quickly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>B00016UX0K</td>\n",
       "      <td>1081555200</td>\n",
       "      <td>Mae Ploy Sweet Chili Sauce is becoming a stand...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10157</th>\n",
       "      <td>B0000VABYE</td>\n",
       "      <td>1082937600</td>\n",
       "      <td>Took along time getting here about 2 1/2 weeks...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>B0001ES9F8</td>\n",
       "      <td>1083110400</td>\n",
       "      <td>The Senseo machine shipped with two 18-pod sam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9589</th>\n",
       "      <td>B0000CEQ6H</td>\n",
       "      <td>1084492800</td>\n",
       "      <td>\"We use and believe in stone milling because n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10632</th>\n",
       "      <td>B0000TVUE6</td>\n",
       "      <td>1085702400</td>\n",
       "      <td>This cheese is wonderful.  It arrived well chi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10539</th>\n",
       "      <td>B000084EJY</td>\n",
       "      <td>1090972800</td>\n",
       "      <td>Again, this is one of those varieties that you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>B000084EK7</td>\n",
       "      <td>1090972800</td>\n",
       "      <td>This one is a great basic food. Whatever is in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14123</th>\n",
       "      <td>B000162MSQ</td>\n",
       "      <td>1092355200</td>\n",
       "      <td>Wow - I dont usually like chocoate in my cooki...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13962</th>\n",
       "      <td>B0000D9N4M</td>\n",
       "      <td>1094860800</td>\n",
       "      <td>This combination is fabulous, from the smoky B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14941</th>\n",
       "      <td>B0001WKFYS</td>\n",
       "      <td>1095811200</td>\n",
       "      <td>After years of imbargos on Iranian products, w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ProductId        Time  \\\n",
       "6986   B00004RYGX  1052265600   \n",
       "10971  B0000TL5HI  1073606400   \n",
       "7040   B0000DG58Q  1074470400   \n",
       "5004   B0001AGLV6  1074643200   \n",
       "6490   B0000DGFAC  1075420800   \n",
       "144    B00016UX0K  1081555200   \n",
       "10157  B0000VABYE  1082937600   \n",
       "5387   B0001ES9F8  1083110400   \n",
       "9589   B0000CEQ6H  1084492800   \n",
       "10632  B0000TVUE6  1085702400   \n",
       "10539  B000084EJY  1090972800   \n",
       "242    B000084EK7  1090972800   \n",
       "14123  B000162MSQ  1092355200   \n",
       "13962  B0000D9N4M  1094860800   \n",
       "14941  B0001WKFYS  1095811200   \n",
       "\n",
       "                                                    Text Score  \n",
       "6986   Embarrassing comedy that comes over like a chi...     0  \n",
       "10971  I purchased the organic fruit basket as a gift...     1  \n",
       "7040   For many years I tried various meat sauces: Am...     1  \n",
       "5004   I got some of this blend thinking that it woul...     1  \n",
       "6490   The plant is very healthy. It arrived quickly ...     0  \n",
       "144    Mae Ploy Sweet Chili Sauce is becoming a stand...     1  \n",
       "10157  Took along time getting here about 2 1/2 weeks...     0  \n",
       "5387   The Senseo machine shipped with two 18-pod sam...     0  \n",
       "9589   \"We use and believe in stone milling because n...     1  \n",
       "10632  This cheese is wonderful.  It arrived well chi...     1  \n",
       "10539  Again, this is one of those varieties that you...     1  \n",
       "242    This one is a great basic food. Whatever is in...     1  \n",
       "14123  Wow - I dont usually like chocoate in my cooki...     1  \n",
       "13962  This combination is fabulous, from the smoky B...     1  \n",
       "14941  After years of imbargos on Iranian products, w...     1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the data has been sorted or not\n",
    "\n",
    "sorted_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the data has been sorted wrt time, now we will only consider the 'Text' and 'Score' columns henceforth\n",
    "\n",
    "sorted_final = sorted_data[['Text','Score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6986</th>\n",
       "      <td>Embarrassing comedy that comes over like a chi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10971</th>\n",
       "      <td>I purchased the organic fruit basket as a gift...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7040</th>\n",
       "      <td>For many years I tried various meat sauces: Am...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>I got some of this blend thinking that it woul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6490</th>\n",
       "      <td>The plant is very healthy. It arrived quickly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text Score\n",
       "6986   Embarrassing comedy that comes over like a chi...     0\n",
       "10971  I purchased the organic fruit basket as a gift...     1\n",
       "7040   For many years I tried various meat sauces: Am...     1\n",
       "5004   I got some of this blend thinking that it woul...     1\n",
       "6490   The plant is very healthy. It arrived quickly ...     0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first few rows of the data to ensure we have the right data\n",
    "\n",
    "sorted_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "I got some of this blend thinking that it would be a little different on steamed vegetables or greens, but when I tried it on eggs, I was astounded!  Used sparingly, it really makes plain old scrambled eggs into a taste treat.<p>The key  with all of these spice blends is to USE SPARINGLY!  I've used most of the other blends as well - a little goes a long way towards creating subtle overtones in your dishes.  Another good one is &quot;Are You Game?&quot;  Mostly I use that for meat dishes, but it's really good in lasagne and spaghetti sauce too.\n"
     ]
    }
   ],
   "source": [
    "# The next task is to clean the text data so that it can be fed to the model\n",
    "\n",
    "# Checking if there are unknown elements in the data\n",
    "\n",
    "# find sentences containing HTML tags\n",
    "\n",
    "import re\n",
    "\n",
    "i=0;\n",
    "for sent in sorted_final['Text'].values:\n",
    "    if (len(re.findall('<.*?>', sent))):\n",
    "        print(i)\n",
    "        print(sent)\n",
    "        break;\n",
    "    i += 1;   \n",
    "    \n",
    "# We can see that the data contains html tags, we will need to remove those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kulkarni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We will perform the data cleaning steps on the text data.\n",
    "For that we will import some packages for stopwords removal, word stemmatization and cleaning html and punctuation marks.\n",
    "'''\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = set(stopwords.words('english')) #set of stopwords\n",
    "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in sorted_final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final['Score'].values)[i] == 'positive': \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                    if(final['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    #print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    #print(\"***********************************************************************\")\n",
    "    \n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulkarni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sorted_final['Cleaned_Text'] = final_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will do the classification based on various representations of the text data i.e BoW, Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Bag of Words text representation to create our first RandomForest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the BoW representation of the text\n",
    "\n",
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "final_counts = count_vect.fit_transform(sorted_final['Cleaned_Text'].values).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the final_tf_idf data to 'X' variable\n",
    "\n",
    "X = final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assinging the score to 'y' variable\n",
    "\n",
    "y = np.array(sorted_final['Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will first split the data into train and test sets, then split the train data in to train and cross-validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Splitting train test with 70:30 ratio\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the train data into cross validation and test datasets\n",
    "\n",
    "X_tr, X_CV, y_tr, y_CV = train_test_split(X_train, y_train, test_size=0.3) # Splitting train cross-val with 70:30 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will be using the GradientBoostingClassifier algorithm to classify the data as positive and negative. Here, we will use  'n_estimator' as the hyperparameter which will take various values to determine the best estimator depending upon the missclassification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for n_estimators = 10 and max depth =  2\n",
      "Miss Classification Error: 0.15663410022061142\n",
      "for n_estimators = 10 and max depth =  3\n",
      "Miss Classification Error: 0.15316734951150335\n",
      "for n_estimators = 10 and max depth =  5\n",
      "Miss Classification Error: 0.14339741569492592\n",
      "for n_estimators = 10 and max depth =  10\n",
      "Miss Classification Error: 0.13803971005357707\n",
      "for n_estimators = 50 and max depth =  2\n",
      "Miss Classification Error: 0.13803971005357707\n",
      "for n_estimators = 50 and max depth =  3\n",
      "Miss Classification Error: 0.12826977623699964\n",
      "for n_estimators = 50 and max depth =  5\n",
      "Miss Classification Error: 0.12763945792625275\n",
      "for n_estimators = 50 and max depth =  10\n",
      "Miss Classification Error: 0.12669398046013236\n",
      "for n_estimators = 100 and max depth =  2\n",
      "Miss Classification Error: 0.12669398046013236\n",
      "for n_estimators = 100 and max depth =  3\n",
      "Miss Classification Error: 0.12322722975102429\n",
      "for n_estimators = 100 and max depth =  5\n",
      "Miss Classification Error: 0.11944531988654272\n",
      "for n_estimators = 100 and max depth =  10\n",
      "Miss Classification Error: 0.11944531988654272\n",
      "for n_estimators = 200 and max depth =  2\n",
      "Miss Classification Error: 0.11944531988654272\n",
      "for n_estimators = 200 and max depth =  3\n",
      "Miss Classification Error: 0.11629372833280804\n",
      "for n_estimators = 200 and max depth =  5\n",
      "Miss Classification Error: 0.11440277340056726\n",
      "for n_estimators = 200 and max depth =  10\n",
      "Miss Classification Error: 0.11440277340056726\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We have taken the following values for 'alpha'\n",
    "estimators = [10,50,100,200]\n",
    "max_depth = [2,3,5,10]\n",
    "accuracy_scores = []\n",
    "\n",
    "for i in estimators:\n",
    "    for j in max_depth:\n",
    "        gbdt = GradientBoostingClassifier(n_estimators = i, max_depth = j)\n",
    "        gbdt.fit(X_tr, y_tr)\n",
    "        pred_bow = gbdt.predict(X_CV)\n",
    "        # evaluate CV accuracy\n",
    "        acc_bow = accuracy_score(y_CV, pred_bow, normalize=True) # * float(100)\n",
    "        accuracy_scores.append(acc_bow)\n",
    "        MissClErrBow = [1-x for x in accuracy_scores]\n",
    "        print(\"for n_estimators =\", i,\"and max depth = \", j)\n",
    "        print('Miss Classification Error:',min(MissClErrBow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that for the estimator value of 200 and max depth of 10 we get the minimum cross validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Gradient Boosting Classifier model with BoW text representation with estimator as 200 and depth as 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the GBDT Classifier for depth = 10 and n_estimators = 200 is 88.550629%\n"
     ]
    }
   ],
   "source": [
    "gbdt_bow_final = GradientBoostingClassifier(n_estimators=200, max_depth = 10)\n",
    "gbdt_bow_final.fit(X_tr, y_tr)\n",
    "pred_bow_final = gbdt_bow_final.predict(X_test)\n",
    "\n",
    "# evaluate CV accuracy\n",
    "acc_bow = accuracy_score(y_test, pred_bow_final, normalize=True) * float(100)\n",
    "\n",
    "print('\\nThe accuracy of the GBDT Classifier for depth = 10 and n_estimators = 200 is %f%%' % (acc_bow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the TF-IDF text representation to create our Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this stage we have initialized the tf-idf vectorizer and applied it to the text data which has been stored in the final_tf_idf vriable\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer()\n",
    "final_tf_idf = tf_idf_vect.fit_transform(sorted_final['Cleaned_Text'].values).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the final_tf_idf data to 'X1' variable\n",
    "\n",
    "X1 = final_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assinging the score to 'y1' variable\n",
    "\n",
    "y1 = np.array(sorted_final['Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.3) # Splitting train test with 70:30 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the train data into train and cross validation sets\n",
    "\n",
    "X_tr1, X_CV1, y_tr1, y_CV1 = train_test_split(X_train1, y_train1, test_size=0.3) # Splitting train cross-val with 70:30 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulkarni\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV accuracy for alpha = 0.00001 is 88%\n",
      "\n",
      "CV accuracy for alpha = 0.00010 is 89%\n",
      "\n",
      "CV accuracy for alpha = 0.00100 is 83%\n",
      "\n",
      "CV accuracy for alpha = 0.10000 is 83%\n",
      "\n",
      "CV accuracy for alpha = 1.00000 is 83%\n",
      "\n",
      "CV accuracy for alpha = 10.00000 is 83%\n",
      "\n",
      "CV accuracy for alpha = 100.00000 is 83%\n",
      "\n",
      "CV accuracy for alpha = 1000.00000 is 83%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We have taken the following values for 'alpha'\n",
    "estimators = [10,50,100,200]\n",
    "max_depth = [2,3,5,10]\n",
    "accuracy_scores_tfidf = []\n",
    "\n",
    "for i in estimators:\n",
    "    for j in max_depth:\n",
    "        gbdt_tfidf = GradientBoostingClassifier(n_estimators = i, max_depth = j)\n",
    "        gbdt_tfidf.fit(X_tr1, y_tr1)\n",
    "        pred_tfidf = gbdt_tfidf.predict(X_CV1)\n",
    "        # evaluate CV accuracy\n",
    "        acc_tfidf = accuracy_score(y_CV1, pred_tfidf, normalize=True) # * float(100)\n",
    "        accuracy_scores_tfidf.append(acc_tfidf)\n",
    "        MissClErrtfidf = [1-x for x in accuracy_scores]\n",
    "        print(\"for n_estimators =\", i,\"and max depth = \", j)\n",
    "        print('Miss Classification Error:',MissClErrtfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that for the alpha value of 0.001 we get the maximum cross validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a SGDClassifier model with TF-IDF text representation with an alpha value of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulkarni\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy for alpha = 0.001 is 89%\n"
     ]
    }
   ],
   "source": [
    "sgd_tfidf = linear_model.SGDClassifier(alpha = 0.0001)\n",
    "sgd_tfidf.fit(X_tr1, y_tr1)\n",
    "pred_acc1 = sgd_tfidf.predict(X_test1)\n",
    "acc_tfidf = accuracy_score(y_test1, pred_acc1, normalize=True) * float(100)\n",
    "print('\\nTest accuracy for alpha = 0.001 is %d%%' % (acc_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the W2Vec representation of the text data to apply the SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulkarni\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Importing the required models for the project\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in sorted_final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if(cleaned_words.isalpha()):    \n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue \n",
    "    list_of_sent.append(filtered_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=50, workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7834\n"
     ]
    }
   ],
   "source": [
    "words = list(w2v_model.wv.vocab)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Avg W2Vec representation of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15109\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# Computing the Avg W2Vec representation of each review and storing it in 'sent_vectors' list\n",
    "\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the list in a variable 'X2'\n",
    "\n",
    "X2 = sent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the review score in a 'y2' variable\n",
    "\n",
    "y2 = sorted_final['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train test sets\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3) # Splitting train test with 70:30 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the train data into train and cross validation sets\n",
    "\n",
    "X_tr2, X_CV2, y_tr2, y_CV2 = train_test_split(X_train2, y_train2, test_size=0.3) # Splitting train cross-val with 70:30 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for n_estimators = 10 and max depth =  2\n",
      "Miss Classification Error: 0.15001575795776867\n",
      "for n_estimators = 10 and max depth =  3\n",
      "Miss Classification Error: 0.15001575795776867\n",
      "for n_estimators = 10 and max depth =  5\n",
      "Miss Classification Error: 0.1465490072486606\n",
      "for n_estimators = 10 and max depth =  10\n",
      "Miss Classification Error: 0.1465490072486606\n",
      "for n_estimators = 50 and max depth =  2\n",
      "Miss Classification Error: 0.14119130160731175\n",
      "for n_estimators = 50 and max depth =  3\n",
      "Miss Classification Error: 0.13803971005357707\n",
      "for n_estimators = 50 and max depth =  5\n",
      "Miss Classification Error: 0.13772455089820357\n",
      "for n_estimators = 50 and max depth =  10\n",
      "Miss Classification Error: 0.13772455089820357\n",
      "for n_estimators = 100 and max depth =  2\n",
      "Miss Classification Error: 0.13772455089820357\n",
      "for n_estimators = 100 and max depth =  3\n",
      "Miss Classification Error: 0.13614875512133628\n",
      "for n_estimators = 100 and max depth =  5\n",
      "Miss Classification Error: 0.13236684525685471\n",
      "for n_estimators = 100 and max depth =  10\n",
      "Miss Classification Error: 0.13236684525685471\n",
      "for n_estimators = 200 and max depth =  2\n",
      "Miss Classification Error: 0.13236684525685471\n",
      "for n_estimators = 200 and max depth =  3\n",
      "Miss Classification Error: 0.13236684525685471\n",
      "for n_estimators = 200 and max depth =  5\n",
      "Miss Classification Error: 0.13142136779073432\n",
      "for n_estimators = 200 and max depth =  10\n",
      "Miss Classification Error: 0.13142136779073432\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We have taken the following values for 'alpha'\n",
    "estimators = [10,50,100,200]\n",
    "max_depth = [2,3,5,10]\n",
    "accuracy_scores_avgw2vec = []\n",
    "\n",
    "for i in estimators:\n",
    "    for j in max_depth:\n",
    "        gbdt_avgw2vec = GradientBoostingClassifier(n_estimators = i, max_depth = j)\n",
    "        gbdt_avgw2vec.fit(X_tr2, y_tr2)\n",
    "        pred_avgw2vec = gbdt_avgw2vec.predict(X_CV2)\n",
    "        # evaluate CV accuracy\n",
    "        acc_avgw2vec = accuracy_score(y_CV2, pred_avgw2vec, normalize=True) # * float(100)\n",
    "        accuracy_scores_avgw2vec.append(acc_avgw2vec)\n",
    "        MissClErravgw2vec = [1-x for x in accuracy_scores_avgw2vec]\n",
    "        print(\"for n_estimators =\", i,\"and max depth = \", j)\n",
    "        print('Miss Classification Error:',min(MissClErravgw2vec))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that for the estimator value of 200 and depth value of 10 we get the minimum miss classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a GBDT Classifier model with avgW2Vec text representation with an estimator value of 200 and max_depth value of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the GBDT Classifier for depth = 10 and n_estimators = 200 is 86.234282%\n"
     ]
    }
   ],
   "source": [
    "gbdt_avgw2vec_final = GradientBoostingClassifier(n_estimators=200, max_depth = 10)\n",
    "gbdt_avgw2vec_final.fit(X_tr2, y_tr2)\n",
    "pred_avgw2vec_final = gbdt_avgw2vec_final.predict(X_test2)\n",
    "\n",
    "# evaluate CV accuracy\n",
    "acc_avgw2vec = accuracy_score(y_test2, pred_avgw2vec_final, normalize=True) * float(100)\n",
    "\n",
    "print('\\nThe accuracy of the GBDT Classifier for depth = 10 and n_estimators = 200 is %f%%' % (acc_avgw2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Tf-Idf weighted W2Vec representation of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulkarni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vect = TfidfVectorizer()\n",
    "final_tf_idf = tf_idf_vect.fit_transform(sorted_final['Cleaned_Text'].values).todense()\n",
    "\n",
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = tf_idf_vect.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "row=0;\n",
    "for sent in list_of_sent: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            tf_idf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            #tfidf = final_tf_idf[row, tfidf_feat.index(word)]\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for any 'NaN' values in the data\n",
    "\n",
    "np.isnan(tfidf_sent_vectors).any()\n",
    "\n",
    "# We can see that there are na values in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will replace all the 'Nan' values w/ mean of the respective columns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "data_tfidf = imp.fit_transform(tfidf_sent_vectors) # Assigning the imputed matrix w/o Nan values to data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if our imputation has succeded\n",
    "\n",
    "np.isnan(data_tfidf).any()\n",
    "\n",
    "# We can see that there are no na values any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the 'data_tfidf' matrix to 'X3' variable\n",
    "\n",
    "X3 = data_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the scores to the 'y3' variable\n",
    "\n",
    "y3 = sorted_final['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train test sets\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.3) # Splitting train test with 70:30 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the train data into train and cross validation sets\n",
    "\n",
    "X_tr3, X_CV3, y_tr3, y_CV3 = train_test_split(X_train3, y_train3, test_size=0.3) # Splitting train cross-val with 70:30 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for n_estimators = 10 and max depth =  2\n",
      "Miss Classification Error: 0.1594705326189726\n",
      "for n_estimators = 10 and max depth =  3\n",
      "Miss Classification Error: 0.1594705326189726\n",
      "for n_estimators = 10 and max depth =  5\n",
      "Miss Classification Error: 0.1588402143082257\n",
      "for n_estimators = 10 and max depth =  10\n",
      "Miss Classification Error: 0.1588402143082257\n",
      "for n_estimators = 50 and max depth =  2\n",
      "Miss Classification Error: 0.1588402143082257\n",
      "for n_estimators = 50 and max depth =  3\n",
      "Miss Classification Error: 0.1588402143082257\n",
      "for n_estimators = 50 and max depth =  5\n",
      "Miss Classification Error: 0.1578947368421053\n",
      "for n_estimators = 50 and max depth =  10\n",
      "Miss Classification Error: 0.1578947368421053\n",
      "for n_estimators = 100 and max depth =  2\n",
      "Miss Classification Error: 0.1578947368421053\n",
      "for n_estimators = 100 and max depth =  3\n",
      "Miss Classification Error: 0.1575795776867318\n",
      "for n_estimators = 100 and max depth =  5\n",
      "Miss Classification Error: 0.15694925937598492\n",
      "for n_estimators = 100 and max depth =  10\n",
      "Miss Classification Error: 0.15694925937598492\n",
      "for n_estimators = 200 and max depth =  2\n",
      "Miss Classification Error: 0.15694925937598492\n",
      "for n_estimators = 200 and max depth =  3\n",
      "Miss Classification Error: 0.15631894106523792\n",
      "for n_estimators = 200 and max depth =  5\n",
      "Miss Classification Error: 0.15631894106523792\n",
      "for n_estimators = 200 and max depth =  10\n",
      "Miss Classification Error: 0.15631894106523792\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We have taken the following values for 'alpha'\n",
    "estimators = [10,50,100,200]\n",
    "max_depth = [2,3,5,10]\n",
    "accuracy_scores_tfidfw2vec = []\n",
    "\n",
    "for i in estimators:\n",
    "    for j in max_depth:\n",
    "        gbdt_tfidfw2vec = GradientBoostingClassifier(n_estimators = i, max_depth = j)\n",
    "        gbdt_tfidfw2vec.fit(X_tr3, y_tr3)\n",
    "        pred_tfidfw2vec = gbdt_tfidfw2vec.predict(X_CV3)\n",
    "        # evaluate CV accuracy\n",
    "        acc_tfidfw2vec = accuracy_score(y_CV3, pred_tfidfw2vec, normalize=True) # * float(100)\n",
    "        accuracy_scores_tfidfw2vec.append(acc_tfidfw2vec)\n",
    "        MissClErrtfidfw2vec = [1-x for x in accuracy_scores_tfidfw2vec]\n",
    "        print(\"for n_estimators =\", i,\"and max depth = \", j)\n",
    "        print('Miss Classification Error:',min(MissClErrtfidfw2vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that for the estimator value of 200 and depth value of 5 we get the minimum miss classification error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a GBDT Classifier model with tfidfW2Vec text representation with an estimator value of 200 and max_depth value of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The accuracy of the GBDT Classifier for depth = 5 and n_estimators = 200 is 84.888595%\n"
     ]
    }
   ],
   "source": [
    "gbdt_tfidfw2vec_final = GradientBoostingClassifier(n_estimators=200, max_depth = 5)\n",
    "gbdt_tfidfw2vec_final.fit(X_tr3, y_tr3)\n",
    "pred_tfidfw2vec_final = gbdt_tfidfw2vec_final.predict(X_test3)\n",
    "\n",
    "# evaluate CV accuracy\n",
    "acc_tfidfw2vec = accuracy_score(y_test3, pred_tfidfw2vec_final, normalize=True) * float(100)\n",
    "\n",
    "print('\\nThe accuracy of the GBDT Classifier for depth = 5 and n_estimators = 200 is %f%%' % (acc_tfidfw2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this problem we have applied the Random Forest Classifier to the data. We have only considered data in the form of BoW, AvgW2Vec and TF-IDF W2Vec.\n",
    "\n",
    "# We have not considered the TF-IDF representation because the time-complexity was very large as observed while running the operations on the data.\n",
    "\n",
    "# Following results have been observed for the data using the available text representations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. BoW representation, depth = 10,n_estimators = 200, accuracy = 88.55%.\n",
    "2. AvgW2Vec representation, depth = 10,n_estimators = 200, accuracy = 86.23%.\n",
    "3. TF-IDFW2Vec representation, depth = 5,n_estimators = 200, accuracy = 84.88%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can observe that we get maximum accuracy for BoW representation of the text data after applying the GBDT algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
