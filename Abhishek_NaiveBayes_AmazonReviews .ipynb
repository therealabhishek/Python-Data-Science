{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFYING AMAZON FOOD REVIEWS USING NAIVE BAYES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#================ OBJECTIVE ======================#\n",
    "\n",
    "Classifying Amazon Food reviews as positive or negative using KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries required for the dataset\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this stage we will be importing the required sqlite dataset\n",
    "\n",
    "con = sqlite3.connect('database.sqlite')\n",
    "\n",
    "'''\n",
    "Our objective is to check whether the review is positive or negative.\n",
    "\n",
    "The dataset originally consists of reviews from 1 to 5. We will consider reviews which are rated as 4 and 5 to be positive, reviews \n",
    "which are rated 1 and 2 to be negative. As, we cannot draw any conclusions from review which is rated 3 star we will eliminate\n",
    "all the reviews rated 3 star.\n",
    "\n",
    "'''\n",
    "\n",
    "# Filtering out the data w/o the 3 star reviews.\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score != 3\"\"\", con) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 2], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking whether the data has been filtered properly\n",
    "\n",
    "filtered_data['Score'].unique()\n",
    "\n",
    "# We can see that there is no 3 star review in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have eliminated the data with 3 start reviews, we will label the remaining data(4 and 5 scores) as positive and negative(1 and 2 scores).\n",
    "\n",
    "# Creating a function to label the data\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return '0'\n",
    "    return '1'\n",
    "\n",
    "\n",
    "# Applying the labels to the data\n",
    "\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "filtered_data['Score'] = positiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>ADT0SRK1MGOEU</td>\n",
       "      <td>Twoapennything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1342051200</td>\n",
       "      <td>Nice Taffy</td>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1SP2KVKFXXRU1</td>\n",
       "      <td>David C. Sullivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A3JRGQVEQN31IQ</td>\n",
       "      <td>Pamela G. Williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1336003200</td>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>B000E7L2R4</td>\n",
       "      <td>A1MZYO9TZK0BBI</td>\n",
       "      <td>R. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1322006400</td>\n",
       "      <td>Yay Barley</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>B00171APVA</td>\n",
       "      <td>A21BT40VZCCYT4</td>\n",
       "      <td>Carol A. Reed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Healthy Dog Food</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "5   6  B006K2ZZ7K   ADT0SRK1MGOEU                   Twoapennything   \n",
       "6   7  B006K2ZZ7K  A1SP2KVKFXXRU1                David C. Sullivan   \n",
       "7   8  B006K2ZZ7K  A3JRGQVEQN31IQ               Pamela G. Williams   \n",
       "8   9  B000E7L2R4  A1MZYO9TZK0BBI                         R. James   \n",
       "9  10  B00171APVA  A21BT40VZCCYT4                    Carol A. Reed   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator Score        Time  \\\n",
       "0                     1                       1     1  1303862400   \n",
       "1                     0                       0     0  1346976000   \n",
       "2                     1                       1     1  1219017600   \n",
       "3                     3                       3     0  1307923200   \n",
       "4                     0                       0     1  1350777600   \n",
       "5                     0                       0     1  1342051200   \n",
       "6                     0                       0     1  1340150400   \n",
       "7                     0                       0     1  1336003200   \n",
       "8                     1                       1     1  1322006400   \n",
       "9                     0                       0     1  1351209600   \n",
       "\n",
       "                                         Summary  \\\n",
       "0                          Good Quality Dog Food   \n",
       "1                              Not as Advertised   \n",
       "2                          \"Delight\" says it all   \n",
       "3                                 Cough Medicine   \n",
       "4                                    Great taffy   \n",
       "5                                     Nice Taffy   \n",
       "6  Great!  Just as good as the expensive brands!   \n",
       "7                         Wonderful, tasty taffy   \n",
       "8                                     Yay Barley   \n",
       "9                               Healthy Dog Food   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  This is a confection that has been around a fe...  \n",
       "3  If you are looking for the secret ingredient i...  \n",
       "4  Great taffy at a great price.  There was a wid...  \n",
       "5  I got a wild hair for taffy and ordered this f...  \n",
       "6  This saltwater taffy had great flavors and was...  \n",
       "7  This taffy is so good.  It is very soft and ch...  \n",
       "8  Right now I'm mostly just sprouting this so my...  \n",
       "9  This is a very healthy dog food. Good for thei...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first few rows of the data\n",
    "\n",
    "filtered_data.head(10)\n",
    "\n",
    "# We can see that the score has been changed to positive and negative instead of 5,4,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us look at the shape of the data\n",
    "\n",
    "filtered_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping any duplicates if they are present in the data\n",
    "\n",
    "duplicates_dropped=filtered_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
    "duplicates_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminating the rows where helpfulness numerator is greator than the helfulness denominator\n",
    "\n",
    "final=duplicates_dropped[duplicates_dropped.HelpfulnessNumerator<=duplicates_dropped.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Taking a look at the shape of the data\n",
    "\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will select only the required columns of which we will filter 100000 reviews and assign them their labels and sort them wrt time\n",
    "\n",
    "final_data = final[['ProductId','Time','Text','Score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the dimension of the data\n",
    "final_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selecting only 15000 reviews from the final_data\n",
    "\n",
    "# First we will extract the values from the given dataframe\n",
    "\n",
    "X=final_data.iloc[:,:].values\n",
    "\n",
    "# randomly extracting 15000 reviews from the dataset\n",
    "\n",
    "import random\n",
    "\n",
    "n = 364171\n",
    "m = 15000\n",
    "p = m/n\n",
    "\n",
    "sampled_data = [];\n",
    "\n",
    "for i in range(0,n):\n",
    "    if random.random() <= p:\n",
    "        sampled_data.append(X[i,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the extracted data to a dataframe\n",
    "\n",
    "names = ['ProductId','Time','Text','Score']\n",
    "\n",
    "sample = pd.DataFrame(sampled_data,columns= names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14754, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the dimensions of the sampled data\n",
    "\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B001GVISJM</td>\n",
       "      <td>1304467200</td>\n",
       "      <td>I love this candy.  After weight watchers I ha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B001EO5TPM</td>\n",
       "      <td>1215302400</td>\n",
       "      <td>Arrived in 6 days and were so stale i could no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0019CW0HE</td>\n",
       "      <td>1333929600</td>\n",
       "      <td>This food is great - all ages dogs.  I have a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B003SE19UK</td>\n",
       "      <td>1324166400</td>\n",
       "      <td>I started buying this after I noticed my 1 yea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B003SE19UK</td>\n",
       "      <td>1330992000</td>\n",
       "      <td>This cat food was recommended by my vet becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId        Time                                               Text  \\\n",
       "0  B001GVISJM  1304467200  I love this candy.  After weight watchers I ha...   \n",
       "1  B001EO5TPM  1215302400  Arrived in 6 days and were so stale i could no...   \n",
       "2  B0019CW0HE  1333929600  This food is great - all ages dogs.  I have a ...   \n",
       "3  B003SE19UK  1324166400  I started buying this after I noticed my 1 yea...   \n",
       "4  B003SE19UK  1330992000  This cat food was recommended by my vet becaus...   \n",
       "\n",
       "  Score  \n",
       "0     1  \n",
       "1     0  \n",
       "2     1  \n",
       "3     1  \n",
       "4     1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first few rows of the data\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will sort the data according to timestamp\n",
    "\n",
    "sorted_data=sample.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4694</th>\n",
       "      <td>B00004S1C6</td>\n",
       "      <td>965779200</td>\n",
       "      <td>These are easy to use, they do not make a mess...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6779</th>\n",
       "      <td>B00004RYGX</td>\n",
       "      <td>966297600</td>\n",
       "      <td>This is such a great film, I don't even know h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4689</th>\n",
       "      <td>B00006L2ZT</td>\n",
       "      <td>1036627200</td>\n",
       "      <td>Well, maybe not &amp;quot;the&amp;quot; greatest, but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>B001O8NLV2</td>\n",
       "      <td>1038009600</td>\n",
       "      <td>For me, when the days get colder nothing is as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6480</th>\n",
       "      <td>B00005IX98</td>\n",
       "      <td>1046044800</td>\n",
       "      <td>After years of using Starbuck's decaf and regu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11603</th>\n",
       "      <td>B0000CA4TK</td>\n",
       "      <td>1068422400</td>\n",
       "      <td>I live in exile in Denver now - I couldn't sur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8988</th>\n",
       "      <td>B0000DIYUK</td>\n",
       "      <td>1069286400</td>\n",
       "      <td>I like this flavor, but it kind of reminds me ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11693</th>\n",
       "      <td>B0000DBN2F</td>\n",
       "      <td>1070582400</td>\n",
       "      <td>Blend a little apple cider mix into your Tazo ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8775</th>\n",
       "      <td>B0000DIVUR</td>\n",
       "      <td>1070755200</td>\n",
       "      <td>My brother-in-law was thrilled with this gift ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10245</th>\n",
       "      <td>B0000DBN1Q</td>\n",
       "      <td>1073433600</td>\n",
       "      <td>\"There is a garden overlooking the Yangtze Riv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9176</th>\n",
       "      <td>B0000SWIVG</td>\n",
       "      <td>1074988800</td>\n",
       "      <td>I ordered the pecan tin as a Christmas present...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13500</th>\n",
       "      <td>B0000DG87B</td>\n",
       "      <td>1082073600</td>\n",
       "      <td>The tree came in great condition, healthy and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11339</th>\n",
       "      <td>B0002CEVP6</td>\n",
       "      <td>1086998400</td>\n",
       "      <td>Yummy,  This was the best I ever had!!  Great ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>B000084F04</td>\n",
       "      <td>1089590400</td>\n",
       "      <td>I was given a box of this when I purchased my ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10158</th>\n",
       "      <td>B000084EJY</td>\n",
       "      <td>1090972800</td>\n",
       "      <td>Again, this is one of those varieties that you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ProductId        Time  \\\n",
       "4694   B00004S1C6   965779200   \n",
       "6779   B00004RYGX   966297600   \n",
       "4689   B00006L2ZT  1036627200   \n",
       "7175   B001O8NLV2  1038009600   \n",
       "6480   B00005IX98  1046044800   \n",
       "11603  B0000CA4TK  1068422400   \n",
       "8988   B0000DIYUK  1069286400   \n",
       "11693  B0000DBN2F  1070582400   \n",
       "8775   B0000DIVUR  1070755200   \n",
       "10245  B0000DBN1Q  1073433600   \n",
       "9176   B0000SWIVG  1074988800   \n",
       "13500  B0000DG87B  1082073600   \n",
       "11339  B0002CEVP6  1086998400   \n",
       "11000  B000084F04  1089590400   \n",
       "10158  B000084EJY  1090972800   \n",
       "\n",
       "                                                    Text Score  \n",
       "4694   These are easy to use, they do not make a mess...     1  \n",
       "6779   This is such a great film, I don't even know h...     1  \n",
       "4689   Well, maybe not &quot;the&quot; greatest, but ...     1  \n",
       "7175   For me, when the days get colder nothing is as...     1  \n",
       "6480   After years of using Starbuck's decaf and regu...     1  \n",
       "11603  I live in exile in Denver now - I couldn't sur...     1  \n",
       "8988   I like this flavor, but it kind of reminds me ...     1  \n",
       "11693  Blend a little apple cider mix into your Tazo ...     1  \n",
       "8775   My brother-in-law was thrilled with this gift ...     1  \n",
       "10245  \"There is a garden overlooking the Yangtze Riv...     1  \n",
       "9176   I ordered the pecan tin as a Christmas present...     0  \n",
       "13500  The tree came in great condition, healthy and ...     1  \n",
       "11339  Yummy,  This was the best I ever had!!  Great ...     1  \n",
       "11000  I was given a box of this when I purchased my ...     1  \n",
       "10158  Again, this is one of those varieties that you...     1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the data has been sorted or not\n",
    "\n",
    "sorted_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the data has been sorted wrt time, now we will only consider the 'Text' and 'Score' columns henceforth\n",
    "\n",
    "sorted_final = sorted_data[['Text','Score']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4694</th>\n",
       "      <td>These are easy to use, they do not make a mess...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6779</th>\n",
       "      <td>This is such a great film, I don't even know h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4689</th>\n",
       "      <td>Well, maybe not &amp;quot;the&amp;quot; greatest, but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>For me, when the days get colder nothing is as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6480</th>\n",
       "      <td>After years of using Starbuck's decaf and regu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Score\n",
       "4694  These are easy to use, they do not make a mess...     1\n",
       "6779  This is such a great film, I don't even know h...     1\n",
       "4689  Well, maybe not &quot;the&quot; greatest, but ...     1\n",
       "7175  For me, when the days get colder nothing is as...     1\n",
       "6480  After years of using Starbuck's decaf and regu...     1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the first few rows of the data to ensure we have the right data\n",
    "\n",
    "sorted_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "For me, when the days get colder nothing is as rewarding as a simple cup of hot tea. And for it's claimed immunity benefits, a basic green tea is a common pick for maintaining a healthy natural balance during the flu season. From previous experiences in tasting the Tazo brand, both of the bottled and boxed products, they have proven to be unsurpassed for quality and flavor. Once I've tried their teas they immediately became my drink of choice. <p>The Zen Green Tea Blend is a wonderful one that has only a few ingredients with no artificial anything. And thankfully, doesn't boast the addition of fortified vitamins in some senseless amount. It truly is an enlightening blend of green tea, spearmint, lemongrass and lemon verbena. Thus making it versatile refreshment for anytime of the day, whether it's right after meals or between meals, or just before bedtime. Generally light and mild tasting, but that will depend upon how long you steep it and if you add a sweetener of some form.<p>Interesting too, are the amusing comments and remarks that appear on the packaging. Reading through this as you drink your tea makes it a distinctive experience. I never seen tea so clever! I wonder if consuming Tazo really does improve a person's outlook on life and affect his or her well being? I think it just could be the great taste and aroma and probably the reassuring thought of doing your health a favor.<br />Try a couple of their teas - they are sure to please!<br />And Tazo Zen Green Tea makes a good choice.\n"
     ]
    }
   ],
   "source": [
    "# The next task is to clean the text data so that it can be fed to the model\n",
    "\n",
    "# Checking if there are unknown elements in the data\n",
    "\n",
    "# find sentences containing HTML tags\n",
    "\n",
    "import re\n",
    "\n",
    "i=0;\n",
    "for sent in sorted_final['Text'].values:\n",
    "    if (len(re.findall('<.*?>', sent))):\n",
    "        print(i)\n",
    "        print(sent)\n",
    "        break;\n",
    "    i += 1;   \n",
    "    \n",
    "# We can see that the data contains html tags, we will need to remove those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kulkarni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We will perform the data cleaning steps on the text data.\n",
    "For that we will import some packages for stopwords removal, word stemmatization and cleaning html and punctuation marks.\n",
    "'''\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = set(stopwords.words('english')) #set of stopwords\n",
    "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in sorted_final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    #print(sent);\n",
    "    sent=cleanhtml(sent) # remove HTMl tags\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final['Score'].values)[i] == 'positive': \n",
    "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                    if(final['Score'].values)[i] == 'negative':\n",
    "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue \n",
    "    #print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    #print(\"***********************************************************************\")\n",
    "    \n",
    "    final_string.append(str1)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulkarni\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "sorted_final['Cleaned_Text'] = final_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will do the classification based on various representations of the text data i.e BoW, Tf-Idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Bag of Words text representation to create our first Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the BoW representation of the text\n",
    "\n",
    "count_vect = CountVectorizer() #in scikit-learn\n",
    "final_counts = count_vect.fit_transform(sorted_final['Cleaned_Text'].values).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the final_tf_idf data to 'X' variable\n",
    "\n",
    "X = final_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assinging the score to 'y' variable\n",
    "\n",
    "y = np.array(sorted_final['Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will first split the data into train and test sets, then split the train data in to train and cross-validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) # Splitting train test with 70:30 ratio\n",
    "\n",
    "# Splitting the train data into cross validation and test datasets\n",
    "\n",
    "X_tr, X_CV, y_tr, y_CV = train_test_split(X_train, y_train, test_size=0.3) # Splitting train cross-val with 70:30 ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will be using the multinomialNB algorithm to classify the data as positive and negative. Here, we will use  'alpha' as the hyperparameter which will take various values to determine the best classifier depending upon the hyperparameter.\n",
    "\n",
    "# We will detemine the best value of the hyperparameter based on the cross validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV accuracy for alpha = 0.00001 is 86%\n",
      "\n",
      "CV accuracy for alpha = 0.00010 is 86%\n",
      "\n",
      "CV accuracy for alpha = 0.00100 is 86%\n",
      "\n",
      "CV accuracy for alpha = 0.10000 is 87%\n",
      "\n",
      "CV accuracy for alpha = 1.00000 is 87%\n",
      "\n",
      "CV accuracy for alpha = 10.00000 is 84%\n",
      "\n",
      "CV accuracy for alpha = 100.00000 is 84%\n",
      "\n",
      "CV accuracy for alpha = 1000.00000 is 84%\n"
     ]
    }
   ],
   "source": [
    "# Importing the multinomial naive bayes algorithm\n",
    "# Importing the library required to calcukate the accuracy\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# We have taken the following values for 'alpha'\n",
    "alpha_values = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\n",
    "\n",
    "for i in alpha_values:\n",
    "    clf = MultinomialNB(alpha= i)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    pred = clf.predict(X_CV)\n",
    "    \n",
    "    # evaluate CV accuracy\n",
    "    acc = accuracy_score(y_CV, pred, normalize=True) * float(100)\n",
    "    print('\\nCV accuracy for alpha = %.5f is %d%%' % (i, acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that for the alpha value of 0.1 we get the maximum cross validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a naive bayes model with BoW text representation with an alpha value of 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy for alpha = 1.0 is 87%\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB(alpha= 0.1)\n",
    "nb.fit(X_tr,y_tr)\n",
    "pred_acc = nb.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred_acc, normalize=True) * float(100)\n",
    "print('\\nTest accuracy for alpha = 1.0 is %d%%' % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that we get a test accuracy of 87% for BoW representation of text data with alpha value of 0.1 for the naive bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the next stage we will use model evaluation techniques such as confusion matrix, precision, recall and f1-score to further evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 425  281]\n",
      " [ 267 3454]]\n"
     ]
    }
   ],
   "source": [
    "# Plotting the confusion matrix for our model\n",
    "\n",
    "print(metrics.confusion_matrix(y_test, pred_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the above confusion matrix we can conclude that:\n",
    "# 425 values are True Negative\n",
    "# 3454 values are True Positive\n",
    "# 281 values are False Positive\n",
    "# 267 values are False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.53      0.57       478\n",
      "          1       0.91      0.94      0.93      2498\n",
      "\n",
      "avg / total       0.87      0.87      0.87      2976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries required to obtain the values of precision, recall and f1 score based on class\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,pred_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the above precision, recall and f1-score table we can draw the following conclusions:\n",
    "\n",
    "# For Class 0(negative):\n",
    "\n",
    "From the precision score we can say that of all the values that were predicted to belong to negative class, 62% actually belong to the negative class.\n",
    "\n",
    "From the recall score we can say that of all the points that belong to negative class, 53% were classified belonging to negative class.\n",
    "\n",
    "# For Class 1(positive):\n",
    "\n",
    "From the precision score we can say that of all the values that were predicted to belong to positive class, 91% actually belong to positive class.\n",
    "\n",
    "From the recall score we can say that of all the points that belong to positive class, 94% were classified belongin to positive class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following conclusions can be drawn from the precision, recall and f1-score\n",
    "# \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this section we will get the most important features depending upon the class which help us predict the class whether it be positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important features for negative class prediction:\n",
      "\n",
      "['aahh' 'kee' 'keebler' 'keefer' 'keemun' 'keen' 'keenan' 'sooon' 'keeper'\n",
      " 'keester' 'kefir' 'keg' 'keifir' 'kellog' 'sooohhh' 'kelp' 'kemper' 'ken'\n",
      " 'kebab' 'kennel' 'keaton' 'kcal' 'karissa' 'karla' 'karlsson' 'soooooooo'\n",
      " 'karo' 'soooooo' 'kashmir' 'kasia' 'kasugai' 'kat' 'katchup' 'kathryn'\n",
      " 'katrina' 'katz' 'kauai' 'kavli' 'kazakhstan' 'sooooo' 'karen' 'kentucki'\n",
      " 'kenyan' 'somet' 'kicker' 'someday' 'kickn' 'som' 'kiddi' 'kiddo' 'solv'\n",
      " 'kielbasa' 'kikkoman' 'kili' 'kilimanjaro' 'solubl' 'solomon' 'kiln'\n",
      " 'kilo' 'kim' 'somethign' 'kenya' 'someway' 'keyword' 'sooner' 'kerala'\n",
      " 'kerig' 'kern' 'kernal' 'soo' 'kerrygold' 'sonni' 'kestekidi' 'soni'\n",
      " 'song' 'sone' 'kevin' 'somth' 'keyboard' 'keychain' 'sommeli' 'kgharri'\n",
      " 'kimbo' 'kaplowi' 'kanteen' 'juan' 'sorrento' 'judah' 'sorguhm'\n",
      " 'judgement' 'judgment' 'judi' 'judici' 'jug' 'juggl' 'juguet' 'sorghum'\n",
      " 'juicer' 'jule' 'julep' 'sorbitol' 'julia' 'jtg' 'sorbitiol' 'jsz'\n",
      " 'sorta' 'sounder' 'soulist' 'joliti' 'jolokai' 'sould' 'jolt' 'jon'\n",
      " 'sought' 'jordan' 'jorim' 'jose' 'joseph' 'jostl' 'souffl' 'journalist'\n",
      " 'souchong' 'jovey' 'joyva' 'kap' 'julien' 'jumbon' 'sooooop' 'kagen'\n",
      " 'soooooooooooo' 'kaia' 'kake' 'kal' 'kalamata' 'kalbi' 'sooooooooooo'\n",
      " 'kalua' 'sooooooooo' 'kameda' 'kamut' 'kandi' 'kane' 'kanin' 'kansa'\n",
      " 'kadhi' 'jumbo' 'kaboom' 'jyoti' 'jummi' 'sorbet']\n",
      "\n",
      "Important features for positive class prediction:\n",
      "\n",
      "['aahh' 'longhair' 'longev' 'longan' 'lombus' 'loma' 'lollypop' 'lolli'\n",
      " 'lola' 'loiter' 'loin' 'locro' 'locker' 'loc' 'lobbi' 'loa' 'livli'\n",
      " 'livingroom' 'livid' 'livabl' 'litterbox' 'litterali' 'litll' 'litigi'\n",
      " 'liston' 'listless' 'listerin' 'lisa' 'longterm' 'liqoric' 'lookinng'\n",
      " 'looney' 'ludicr' 'lucuma' 'lthink' 'ltd' 'lrger' 'lozeng' 'lox' 'lowri'\n",
      " 'lowrey' 'loveli' 'loung' 'loumidi' 'louisianna' 'louis' 'louder' 'lotta'\n",
      " 'loti' 'loreal' 'lopsid' 'lopez' 'loosli' 'looser' 'looseleaf' 'loooooov'\n",
      " 'looooooov' 'loooooooov' 'looooong' 'loom' 'luggag' 'lipsmack' 'lipid'\n",
      " 'lickiti' 'licketi' 'licious' 'licens' 'lice' 'librari' 'liberti' 'lib'\n",
      " 'liabl' 'liabil' 'lewi' 'levu' 'leukemia' 'letta' 'lethargi' 'letharg'\n",
      " 'letdown' 'lesssssss' 'lesion' 'leroux' 'leon' 'lent' 'lenni' 'lengthwis'\n",
      " 'lemoney' 'lello' 'lekitho' 'licorac' 'lipitor' 'liederkranz' 'lifter'\n",
      " 'linzer' 'linvill' 'linse' 'linoleum' 'linhart' 'zyrtec' 'lingnan'\n",
      " 'lindi' 'lincoln' 'limon' 'limitless' 'limey' 'limeston' 'limbo'\n",
      " 'limbaugh' 'lillipop' 'lilikoi' 'lilbugg' 'likey' 'liketi' 'liken'\n",
      " 'likeabl' 'lijjat' 'liitl' 'lightbulb' 'ligh' 'ligament' 'lieg' 'lei'\n",
      " 'lumanconi' 'lumpless' 'marthastewart' 'marshal' 'marriott' 'marnoto'\n",
      " 'marki' 'marketspic' 'mario' 'marilyn' 'marich' 'margherita' 'mardol'\n",
      " 'mardinad' 'marcona' 'marca' 'marc' 'marblehead' 'marakesh' 'mar' 'mappl'\n",
      " 'mapo' 'mapley' 'maofeng' 'manyof' 'manur' 'manuev']\n"
     ]
    }
   ],
   "source": [
    "neg_class_prob_sorted = nb.feature_log_prob_[0, :].argsort()\n",
    "pos_class_prob_sorted = nb.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "print('Important features for negative class prediction:\\n')\n",
    "print(np.take(count_vect.get_feature_names(), neg_class_prob_sorted[:150])) # This will show the top 150 features for negative class prediction\n",
    "\n",
    "print('\\nImportant features for positive class prediction:\\n')\n",
    "print(np.take(count_vect.get_feature_names(), pos_class_prob_sorted[:150])) # This will show the top 150 features for positive class prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the TF-IDF text representation to create our Naive Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this stage we have initialized the tf-idf vectorizer and applied it to the text data which has been stored in the final_tf_idf vriable\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer()\n",
    "final_tf_idf = tf_idf_vect.fit_transform(sorted_final['Cleaned_Text'].values).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the final_tf_idf data to 'X1' variable\n",
    "\n",
    "X1 = final_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assinging the score to 'y1' variable\n",
    "\n",
    "y1 = np.array(sorted_final['Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will first split the data into train and test sets, then split the train data in to train and cross-validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.3) # Splitting train test with 70:30 ratio\n",
    "\n",
    "# Splitting the train data into train and cross validation sets\n",
    "\n",
    "X_tr1, X_CV1, y_tr1, y_CV1 = train_test_split(X_train1, y_train1, test_size=0.3) # Splitting train cross-val with 70:30 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV accuracy for i = 0.00001 is 76%\n",
      "\n",
      "CV accuracy for i = 0.00010 is 75%\n",
      "\n",
      "CV accuracy for i = 0.00100 is 75%\n",
      "\n",
      "CV accuracy for i = 0.10000 is 73%\n",
      "\n",
      "CV accuracy for i = 1.00000 is 78%\n",
      "\n",
      "CV accuracy for i = 10.00000 is 83%\n",
      "\n",
      "CV accuracy for i = 100.00000 is 83%\n",
      "\n",
      "CV accuracy for i = 1000.00000 is 83%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "alpha = [0.00001, 0.0001, 0.001, 0.1, 1, 10, 100,1000]\n",
    "\n",
    "for i in alpha:\n",
    "    clf1 = MultinomialNB(alpha= i)\n",
    "    clf1.fit(X_tr, y_tr)\n",
    "    pred1 = clf1.predict(X_CV)\n",
    "    \n",
    "    # evaluate CV accuracy\n",
    "    acc1 = accuracy_score(y_CV1, pred1, normalize=True) * float(100)\n",
    "    print('\\nCV accuracy for i = %.5f is %d%%' % (i, acc1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that for the alpha value of 10 we get the maximum cross validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy for alpha = 0.1 is 89%\n"
     ]
    }
   ],
   "source": [
    "nb1 = MultinomialNB(alpha= 10)\n",
    "nb1.fit(X_tr1,y_tr1)\n",
    "pred2 = nb.predict(X_test1)\n",
    "acc2 = accuracy_score(y_test1, pred2, normalize=True) * float(100)\n",
    "print('\\nTest accuracy for alpha = 10 is %d%%' % (acc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can see that we get a test accuracy of 89% for TF-IDF representation of text data with alpha value of 10 for the naive bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the next stage we will use model evaluation techniques such as confusion matrix, precision, recall and f1-score to further evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.42      0.57       705\n",
      "          1       0.90      0.99      0.94      3722\n",
      "\n",
      "avg / total       0.90      0.90      0.88      4427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test1,pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the above precision, recall and f1-score table we can draw the following conclusions:\n",
    "\n",
    "# For Class 0(negative):\n",
    "\n",
    "From the precision score we can say that of all the values that were predicted to belong to negative class, 89% actually belong to negative class.\n",
    "\n",
    "From the recall score we can say that of all the points that belong to negative class, 42% were classified belonging to negative class.\n",
    "\n",
    "# For Class 1(positive):\n",
    "\n",
    "From the precision score we can say that of all the values that were predicted to belong to positive class, 90% actually belong to positive class.\n",
    "\n",
    "From the recall score we can say that of all the points that belong to positive class, 99% were classified belonging to positive class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 296  409]\n",
      " [  37 3685]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "print(metrics.confusion_matrix(y_test1, pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the above confusion matrix we can conclude that:\n",
    "# 296 values are True Negative\n",
    "# 3685 values are True Positive\n",
    "# 409 values are False Positive\n",
    "# 37 values are False Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this section we will get the most important features depending upon the class which help us predict the class whether it be positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important features for negative class prediction:\n",
      "\n",
      "['aahh' 'odin' 'odorless' 'odour' 'odyssey' 'oem' 'oetker' 'offal'\n",
      " 'offbeat' 'offens' 'odifer' 'officinal' 'ofter' 'ogan' 'oh' 'ohara' 'ohh'\n",
      " 'ohio' 'oiko' 'oild' 'ointment' 'oftentim' 'oder' 'odel' 'odea' 'oblong'\n",
      " 'obnoxi' 'obscen' 'obsolet' 'obsolut' 'obssess' 'ocass' 'ocassion'\n",
      " 'occais' 'occasiona' 'occassion' 'occate' 'occuoi' 'occupi' 'occuppi'\n",
      " 'oceanfish' 'ocha' 'octan' 'octav' 'octavia' 'octob' 'oist' 'okc' 'okla'\n",
      " 'oklahoma' 'omnipres' 'ona' 'oncologist' 'onecup' 'oneil' 'ongo' 'onhand'\n",
      " 'oniion' 'onlyrec' 'onscreen' 'onth' 'ontim' 'oohh' 'oomph' 'oonc' 'ooo'\n",
      " 'oooh' 'ooooh' 'oooooo' 'oopen' 'opertun' 'omni' 'oblig' 'omlett'\n",
      " 'omgood' 'okonomiyaki' 'olak' 'olbus' 'olday' 'oldi' 'ole' 'olean'\n",
      " 'oleoresin' 'olio' 'oliven' 'oliveri' 'olivio' 'olli' 'oma' 'omaha'\n",
      " 'omami' 'omega' 'omelet' 'omelett' 'omerga' 'omgi' 'omit' 'opiat'\n",
      " 'objection' 'obey' 'noy' 'nozzl' 'nsa' 'nuanc' 'nubbi' 'nuclear' 'nudg'\n",
      " 'nueva' 'nuf' 'noxious' 'nuff' 'nugit' 'nugo' 'num' 'numb' 'numero'\n",
      " 'nummi' 'nunatur' 'nupro' 'nurish' 'nugget' 'nowaday' 'novocain' 'novic'\n",
      " 'norm' 'norman' 'norseman' 'north' 'northeast' 'northern' 'northwest'\n",
      " 'norway' 'nos' 'nosh' 'nostalg' 'nostra' 'nostril' 'notat' 'notebook'\n",
      " 'noteworthi' 'notori' 'nougat' 'nov' 'novel' 'novelti' 'nurser' 'nurtur'\n",
      " 'nutcrack' 'nute' 'nuttzo']\n",
      "\n",
      "Important features for positive class prediction:\n",
      "\n",
      "['aahh' 'listless' 'listerin' 'lisa' 'liquoric' 'liquiud' 'liqoric'\n",
      " 'lipsmack' 'lipitor' 'lipid' 'linzer' 'linzano' 'linvill' 'linkett'\n",
      " 'linhart' 'lingoit' 'lingo' 'lingnan' 'lindi' 'limeston' 'limbo'\n",
      " 'limbaugh' 'lillipop' 'lilikoi' 'lili' 'lilbugg' 'likey' 'liketi'\n",
      " 'liston' 'likeabl' 'literatur' 'litigi' 'looser' 'looseleaf' 'looov'\n",
      " 'loooooov' 'looooooov' 'loom' 'lookinng' 'longhair' 'longev' 'longan'\n",
      " 'lombus' 'lollypop' 'lolli' 'lola' 'loiter' 'loin' 'loca' 'loan' 'loa'\n",
      " 'lmoa' 'lloyd' 'livid' 'livabl' 'litterbox' 'litterali' 'litmus' 'litll'\n",
      " 'litig' 'lijjat' 'lignan' 'lightweight' 'lekitho' 'lei' 'legitim'\n",
      " 'legendari' 'legend' 'leganza' 'legandari' 'leek' 'lechera' 'lebanes'\n",
      " 'leas' 'learnt' 'leari' 'leaker' 'leafi' 'leack' 'laziza' 'lazazza'\n",
      " 'layton' 'layoff' 'laxaclear' 'lawyer' 'lawsuit' 'lawnmow' 'lavish'\n",
      " 'lavened' 'lavazzo' 'lello' 'lemoney' 'lengthi' 'lent' 'lightbulb' 'ligh'\n",
      " 'ligament' 'lifter' 'lifestay' 'lifesourc' 'liekt' 'licorac' 'lickiti'\n",
      " 'lickin' 'licketi' 'licious' 'licens' 'loosli' 'lice' 'liberti' 'libat'\n",
      " 'lib' 'liabil' 'levi' 'leukemia' 'letta' 'lethargi' 'letharg' 'letdown'\n",
      " 'lesion' 'leonida' 'leon' 'libr' 'lopez' 'lopsid' 'loser' 'mammal'\n",
      " 'mamad' 'malto' 'maltitol' 'mallow' 'mallo' 'malic' 'malibu' 'malfunct'\n",
      " 'maldon' 'malcom' 'malcolm' 'malaysian' 'malaysia' 'malamut' 'maladi'\n",
      " 'malabsorpt' 'makret' 'makhani' 'makerswher' 'maiz' 'mainten' 'maintanc'\n",
      " 'maim']\n"
     ]
    }
   ],
   "source": [
    "neg_class_prob_sorted1 = nb1.feature_log_prob_[0, :].argsort()\n",
    "pos_class_prob_sorted1 = nb1.feature_log_prob_[1, :].argsort()\n",
    "\n",
    "print('Important features for negative class prediction:\\n')\n",
    "print(np.take(tf_idf_vect.get_feature_names(), neg_class_prob_sorted1[:150])) # This will show the top 150 features for negative class prediction\n",
    "\n",
    "print('\\nImportant features for positive class prediction:\\n')\n",
    "print(np.take(tf_idf_vect.get_feature_names(), pos_class_prob_sorted1[:150])) # This will show the top 150 features for positive class prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For this problem we were only used 2 types representation of the text data i.e Bow and TF-IDF. \n",
    "\n",
    "# We can see that for TF_IDF  representation of the data for alpha value of 10 we get maximum accuracy of 89%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
